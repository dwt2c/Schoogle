This is a list of things that are currently being implemented
	- iterative implementation of pagerank in C++ using the libpq library for postgreql access and modification
		- this will reqire the addition of a new perminante table in our database to be used by this function to calculate pageranks. Despite the large size of the datatable this function will be approximatly O(Plog(p))
		where P is the number of pages logged from our scraping
	- pipeline functions for scrapy.py, to log data through ssh onto mathcs server
	- Textsearch, much of this functionality as well as some basic lexers are implemented in Postgresql so this will not need to be completely built. 


Code for objects made so far is below:
# -*- coding: utf-8 -*-

# Define here the models for your scraped items

import scrapy

#This class is simply a data structure to be filled out by O_Spider during it's scraping run

class O_Item(scrapy.Item):
	""" An item that will hold the content scraped by O_Spider """
	#below we initialize all the fields that will be filled during scraping
	url = scrapy.Field()
	title = scrapy.Field()
	links = scrapy.Field()
	timestamp = scrapy.Field()
	page_size = scrapy.Field()
	full_html = scrapy.Field()
	full_text = scrapy.Field()
	secure = scrapy.Field()


# -*- coding: utf-8 -*-

# this still needs to be implemented, this should take the O_items that are 
# created by our spider and put ship them off to the postgresql server

class O_SpiderPipeline(object):
    def process_item(self, item, spider):
        return item

    
  
from scrapy.spiders import Spider
from scrapy.selector import Selector
from scrapy import Request
from schoogle_spider.items import O_Item
from sys import getsizeof
from datetime import datetime
import time

#@params:
#@html_list: this is list of html in a "List"(aka vector), we replace all of those annoying
#	\t and \n's in clunkey html and return a string with the pages entire html contents,
# 	this object will later be used by postgreql for a full text search.
def prune(html_list):
	for line in html_list:
		line.replace("\t","").replace("\n","")
	return " ".join(html_list)

class O_Spider(Spider):
	name = 'O_Spider'
	allowed_domains = ['owu.edu']
	start_urls = ['http://www.owu.edu']

	# @params
	# @response: this is a Scrapy.Response object containing much of the website information
	# 				attibutes of this object will be used to flesh out our O_Item object
	# @yield(1): this returns a single object each time next( this object ) is called
	# first parse yields all items
	# @yield(2): this is completed only after we have yielded an object from this webpage, it will 
	# recursively call parse on all links in a web page
	def parse(self,response):
		current_item = O_Item()
		# fill up item with statistics
		current_item['url'] = response.url
		current_item['title'] = response.xpath('//title').extract()
		current_item['links'] = response.xpath('//@href').extract()
		current_item['timestamp'] = datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')
		current_item['page_size'] = getsizeof(response.body)
		current_item['full_html'] = response.body
		current_item['full_text'] = prune(response.body)
		current_item['secure'] = 'https' in str(response.request)
		yield current_item


		# recursive page search is below, this must happen after the item is pipelined to postgresql
		try:
			for link in current_item['links']:
				try:
					yield Request(link,callback = self.parse)
				except ValueError:
					pass # might want to log these eventually
		except AttributeError:
			pass # log these eventually
	
